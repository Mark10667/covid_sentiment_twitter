{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read us cities csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv('uscities.csv', delimiter=\",\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User location filter \n",
    "# By testing with big enough samples:\n",
    "# Aronud 8% of tweets have infomation of the user's state\n",
    "# Less than 0.5% of the tweets contain information like tweet's coordinates or user's place\n",
    "# Therefore, for now, we focus on the states information of the tweets \n",
    "\n",
    "# def loc_filter(loc):\n",
    "#     if ',' in loc:\n",
    "#         s1 = loc.split(',')[0].strip().upper()\n",
    "#         s2 = loc.split(',')[1].strip().upper()\n",
    "# #         check if part before ',' is CA for instance \n",
    "#         if(df['state_id'].apply(lambda state_id : state_id==s1).any()):\n",
    "#             return(s1)\n",
    "# #         check if part after ',' is CA for instance \n",
    "#         elif(df['state_id'].apply(lambda state_id : state_id==s2).any()):\n",
    "#             return(s2)\n",
    "# #         check if part before ',' is CALIFORNIA for instance \n",
    "#         elif(df['state_name'].apply(lambda state_name : state_name.upper()==s1).any()):\n",
    "#             return(s1)\n",
    "# #         check if part after ',' is CALIFORNIA for instance \n",
    "#         elif(df['state_name'].apply(lambda state_name : state_name.upper()==s2).any()):\n",
    "#             return(s2)\n",
    "#     else: \n",
    "#         return None \n",
    "        \n",
    "# loc_filter('xxx, VA')\n",
    "\n",
    "\n",
    "# dictionary {\"NY\": New York, \"NEW YORK\": New York}\n",
    "df = pd.read_csv('uscities.csv', delimiter=\",\")\n",
    "dic1 = pd.Series(df.state_name.values,index=df.state_id).to_dict()\n",
    "dic2 = {v.upper(): v for k, v in dic1.items()}\n",
    "\n",
    "\n",
    "# this is the updated version \n",
    "def loc_filter(loc):\n",
    "    if ',' in loc:\n",
    "        s1 = loc.split(',')[0].strip().upper()\n",
    "        s2 = loc.split(',')[1].strip().upper()\n",
    "#         check if part before ',' is CA for instance \n",
    "        if(df['state_id'].apply(lambda state_id : state_id==s1).any()):\n",
    "            return(dic1[s1])\n",
    "#         check if part after ',' is CA for instance \n",
    "        elif(df['state_id'].apply(lambda state_id : state_id==s2).any()):\n",
    "            return(dic1[s2])\n",
    "#         check if part before ',' is CALIFORNIA for instance \n",
    "        elif(df['state_name'].apply(lambda state_name : state_name.upper()==s1).any()):\n",
    "            return(dic2[s1])\n",
    "#         check if part after ',' is CALIFORNIA for instance \n",
    "        elif(df['state_name'].apply(lambda state_name : state_name.upper()==s2).any()):\n",
    "            return(dic2[s2])\n",
    "    else: \n",
    "        return None \n",
    "        \n",
    "loc_filter('xxx, VA')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka Consumer Part\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "# new csv file \n",
    "from csv import DictWriter\n",
    "headersCSV = ['text','location']  \n",
    "\n",
    "# topic_name = 'twitter'\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "     'twitter',\n",
    "     bootstrap_servers=['localhost:9092'],\n",
    "     auto_offset_reset='earliest',\n",
    "     enable_auto_commit=True,\n",
    "#      auto_commit_interval_ms =  5000,\n",
    "#      fetch_max_bytes = 12800,\n",
    "#      max_poll_records = 10000,\n",
    "#      value_deserializer=lambda x: x.decode('utf-8'))\n",
    "    \n",
    "# consumer loads json into dictionary \n",
    "     value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n",
    "\n",
    "\n",
    "with open('geo_tweet_470gcp.csv', 'a', newline='') as f_object:\n",
    "    # Pass the CSV  file object to the Dictwriter() function\n",
    "    # Result - a DictWriter object\n",
    "    dictwriter_object = DictWriter(f_object, fieldnames=headersCSV)\n",
    "    # Pass the data in the dictionary as an argument into the writerow() function\n",
    "    \n",
    "    for message in consumer:\n",
    "        tweets = message.value\n",
    "        if loc_filter(tweets['user_location']) != None:\n",
    "            t = {\n",
    "#               add rstrip?\n",
    "                'text': tweets['text'],\n",
    "                'location': loc_filter(tweets['user_location'])\n",
    "            }\n",
    "            print(t)\n",
    "            \n",
    "            dictwriter_object.writerow(t)\n",
    "    # Close the file object\n",
    "    f_object.close()\n",
    "\n",
    "    \n",
    "# geo_tweet_440.csv are filtered from 71757 tweets in corona_tweets_440.csv\n",
    "# geo_tweet_441.csv are filtered from 73450 tweets in corona_tweets_441.csv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from csv import DictWriter\n",
    "# headersCSV = ['text','location']      \n",
    "# new={'text': 'RT @UberFacts: The U.S. has recorded fewer than 6,000 new COVID cases for the first time since March 2020', 'location': 'TX'}\n",
    "# with open('geo_twe.csv', 'a', newline='') as f_object:\n",
    "#     # Pass the CSV  file object to the Dictwriter() function\n",
    "#     # Result - a DictWriter object\n",
    "#     dictwriter_object = DictWriter(f_object, fieldnames=headersCSV)\n",
    "#     # Pass the data in the dictionary as an argument into the writerow() function\n",
    "#     dictwriter_object.writerow(new)\n",
    "#     # Close the file object\n",
    "#     f_object.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try delete any null row in 470gcp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('geo_tweet_468.csv')\n",
    "df.columns = [\"TWEET\", \"LOCATION\", \"senti\"]\n",
    "# df\n",
    "# fil = (df[\"TWEET\"] != \"\") & (df[\"TWEET\"] != np.nan)\n",
    "# dfNew = df[fil]\n",
    "dfNew[\"TWEET\"] = dfNew[\"TWEET\"].str.replace(\"\\n\",\" \")\n",
    "dfNew[[\"TWEET\", \"LOCATION\"]].head(1000).to_csv('geo_tweet_470gcp.csv', index=False)\n",
    "\n",
    "# dfNew[[\"TWEET\", \"LOCATION\"]].to_csv('geo_tweet_470gcp.csv', index=False)\n",
    "pd.options.display.max_colwidth = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('geo_tweet_470gcp.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for checking \n",
    "from google.cloud import storage\n",
    "\n",
    "def implicit():\n",
    "\n",
    "    # If you don't specify credentials when constructing the client, the\n",
    "    # client library will look for credentials in the environment.\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Make an authenticated API request\n",
    "    buckets = list(storage_client.list_buckets())\n",
    "    print(buckets)\n",
    "implicit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload csv file into GCP bucket, so that it can be loaded into bigquery \n",
    "from google.cloud import storage\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"\" \n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "buckets = list(storage_client.list_buckets())\n",
    "\n",
    "bucket = storage_client.get_bucket(\"tweetid_csv\")\n",
    "\n",
    "blob = bucket.blob('all_in_one.csv')\n",
    "blob.upload_from_filename('all_in_one.csv')\n",
    "print(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv file into the table\n",
    "# table has tweet, location, date \n",
    "# https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#python_1\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# TODO(developer): Set table_id to the ID of the table to create.\n",
    "# table_id = \"your-project.your_dataset.your_table_name\n",
    "\n",
    "# This is the new csv file part \n",
    "\n",
    "uri = \"https://storage.cloud.google.com/tweetid_csv/all_in_one.csv\"\n",
    "table_id = \"tweetanalysis-332220.geotweet.tweet\"\n",
    "\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     schema=[\n",
    "# #         bigquery.SchemaField(\"DATE\", \"DATE\"),\n",
    "#         bigquery.SchemaField(\"TWEET\", \"STRING\"),\n",
    "#         bigquery.SchemaField(\"LOCATION\", \"STRING\"),\n",
    "#     ],\n",
    "#     skip_leading_rows=0,\n",
    "#     # The source format defaults to CSV, so the line below is optional.\n",
    "#     source_format=bigquery.SourceFormat.CSV,\n",
    "# )\n",
    "\n",
    "# client.load_table_from_uri(uri, table_id, job_config=job_config).result()\n",
    "# previous_rows = client.get_table(table_id).num_rows\n",
    "# print(previous_rows)\n",
    "# assert previous_rows > 0\n",
    "\n",
    "# This is the part that we load our data to \n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=[\n",
    "        bigquery.SchemaField(\"TWEET\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"LOCATION\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"DATE\", \"DATE\"),\n",
    "    ],\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,\n",
    ")\n",
    "\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, table_id, job_config=job_config\n",
    ")  # Make an API request.\n",
    "\n",
    "load_job.result()  # Waits for the job to complete.\n",
    "\n",
    "destination_table = client.get_table(table_id)\n",
    "print(\"Loaded {} rows.\".format(destination_table.num_rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we upload file into GCP bucket and also BigQuery, we delete file \n",
    "import os\n",
    "os.remove(\"for_delete.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is reponsible for convert all of the geo_tweet csv files into bigquery on cloud \n",
    "\n",
    "# import pandas as pd \n",
    "# import shutil\n",
    "\n",
    "# # generate names of previous geo tweet csv files \n",
    "# csv_names = []\n",
    "# for i in range(36):\n",
    "#     index = 434+i\n",
    "#     csv_names.append('geo_tweet_' + str(index) + '.csv')\n",
    "\n",
    "    \n",
    "    \n",
    "# # based on previous geo_tweet csv files, add date and unify format of state names \n",
    "# def new_geo(file):\n",
    "#     index = sint(file.split('.')[0].split('_')[-1]) - 434\n",
    "\n",
    "#     df = pd.read_csv(file)\n",
    "#     df[\"location\"].replace(dic1, inplace=True)\n",
    "#     df[\"location\"].replace(dic2, inplace=True)\n",
    "\n",
    "#     if index < 6:\n",
    "#         df['date'] = '2021-05-' + str(26+index)\n",
    "#     elif index < 15: \n",
    "#         df['date'] = '2021-06-0' + str(index-5)\n",
    "#     else: \n",
    "#         df['date'] = '2021-06-' + str(index-5)\n",
    "#     df['date']= pd.to_datetime(df['date'])\n",
    "    \n",
    "#     df = df[['text', 'location', 'date']]\n",
    "    \n",
    "#     df[\"text\"] = df[\"text\"].str.replace(\"\\n\",\" \")\n",
    "    \n",
    "#     df.to_csv(\"new_\"+file)\n",
    "\n",
    "#     source = \"new_\"+file\n",
    "#     destination = \"new_geo_tweet\"\n",
    "\n",
    "#     new_path = shutil.move(source, destination)\n",
    "#     print(new_path)\n",
    "    \n",
    "    \n",
    "# # create new geo files \n",
    "# for file in csv_names:\n",
    "#     new_geo(file)\n",
    "    \n",
    "# # create list of names \n",
    "# new_csv_names = []\n",
    "# for i in range(36):\n",
    "#     index = 434+i\n",
    "#     new_csv_names.append('new_geo_tweet/new_geo_tweet_' + str(index) + '.csv')\n",
    "# new_csv_names\n",
    "\n",
    "# # concatenate the new_geo_tweet files and create the all in one file!!!! columns : [text, location, date]\n",
    "# df_csv = pd.concat(pd.read_csv(file) for file in new_csv_names)\n",
    "# new_df_csv = df_csv[[\"text\", \"location\", \"date\"]].rename(columns={\"text\": \"TWEET\", \"location\": \"LOCATION\", \"date\": \"DATE\"})\n",
    "# new_df_csv[[\"TWEET\", \"LOCATION\", \"DATE\"]].to_csv(\"all_in_one.csv\", index=False)\n",
    "\n",
    "# # check to see how many columns \n",
    "# df = pd.read_csv(\"all_in_one.csv\")\n",
    "# len(df. columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GCP BigQuery table into a dataframe \n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "bqclient = bigquery.Client()\n",
    "\n",
    "# Download a table.\n",
    "table = bigquery.TableReference.from_string(\n",
    "    \"tweetanalysis-332220.geotweet.tweet\"\n",
    ")\n",
    "rows = bqclient.list_rows(\n",
    "    table,\n",
    "    selected_fields=[\n",
    "        bigquery.SchemaField(\"TWEET\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"LOCATION\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"DATE\", \"DATE\"),\n",
    "    ],\n",
    ")\n",
    "dataframe = rows.to_dataframe(\n",
    "    # Optionally, explicitly request to use the BigQuery Storage API. As of\n",
    "    # google-cloud-bigquery version 1.26.0 and above, the BigQuery Storage\n",
    "    # API is used by default.\n",
    "    create_bqstorage_client=True,\n",
    ")\n",
    "\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"TWEET\"].head(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
